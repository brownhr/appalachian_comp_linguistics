
@article{ross2020,
  title = {Dirichletprocess: {{An R Package}} for {{Fitting Complex Bayesian Nonparametric Models}}},
  author = {Ross, Gordon J and Markwick, Dean},
  year = {2020},
  month = jun,
  pages = {42},
  abstract = {The dirichletprocess package provides software for creating flexible Dirichlet processes objects in R. Users can perform nonparametric Bayesian analysis using Dirichlet processes without the need to program their own inference algorithms. Instead, the user can utilise our pre-built models or specify their own models whilst allowing the dirichletprocess package to handle the Markov chain Monte Carlo sampling. Our Dirichlet process objects can act as building blocks for a variety of statistical models including and not limited to: density estimation, clustering and prior distributions in hierarchical models.},
  langid = {english},
  keywords = {â›” No DOI found},
  file = {C\:\\Users\\brownhr\\Zotero\\storage\\WKRH7X6V\\ross2020.pdf}
}

@inproceedings{wallach2006,
  title = {Topic Modeling: Beyond Bag-of-Words},
  shorttitle = {Topic Modeling},
  booktitle = {Proceedings of the 23rd International Conference on {{Machine}} Learning  - {{ICML}} '06},
  author = {Wallach, Hanna M.},
  year = {2006},
  pages = {977--984},
  publisher = {{ACM Press}},
  address = {{Pittsburgh, Pennsylvania}},
  doi = {10.1145/1143844.1143967},
  abstract = {Some models of textual corpora employ text generation methods involving n-gram statistics, while others use latent topic variables inferred using the ``bag-of-words'' assumption, in which word order is ignored. Previously, these methods have not been combined. In this work, I explore a hierarchical generative probabilistic model that incorporates both n-gram statistics and latent topic variables by extending a unigram topic model to include properties of a hierarchical Dirichlet bigram language model. The model hyperparameters are inferred using a Gibbs EM algorithm. On two data sets, each of 150 documents, the new model exhibits better predictive accuracy than either a hierarchical Dirichlet bigram language model or a unigram topic model. Additionally, the inferred topics are less dominated by function words than are topics discovered using unigram statistics, potentially making them more meaningful.},
  isbn = {978-1-59593-383-6},
  langid = {english},
  file = {C\:\\Users\\brownhr\\Zotero\\storage\\LDMB99G4\\wallach2006.pdf}
}

@inproceedings{wallach2009,
  title = {Evaluation Methods for Topic Models},
  booktitle = {Proceedings of the 26th Annual International Conference on Machine Learning},
  author = {Wallach, Hanna M. and Murray, Iain and Salakhutdinov, Ruslan and Mimno, David},
  year = {2009},
  pages = {1105--1112},
  doi = {10.1145/1553374.1553515},
  file = {C\:\\Users\\brownhr\\Zotero\\storage\\8LBCQZUX\\wallach2009.pdf}
}


